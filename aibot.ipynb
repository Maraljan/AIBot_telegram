{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#### Import libraries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "import requests\n",
    "import bs4 as bs\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Creating the Corpus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "response = requests.get('https://en.wikipedia.org/wiki/Tennis')\n",
    "raw_html = response.text\n",
    "\n",
    "article_html = bs.BeautifulSoup(raw_html, 'html')\n",
    "article_paragraphs = article_html.find_all('p')\n",
    "\n",
    "article_text = ''.join(p.text for p in article_paragraphs).lower()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tennis is a racket sport that\n"
     ]
    }
   ],
   "source": [
    "print(article_text[:30])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Text Preprocessing\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tennis is a racket sport that can be played indiv\n"
     ]
    }
   ],
   "source": [
    "article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n",
    "article_text = re.sub(r'\\s+', ' ', article_text)\n",
    "print(article_text[:50])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Divide to sentence\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tennis is a racket sport that can be played individually against a single opponent (singles) or between two teams of two players each (doubles).\n",
      "each player uses a tennis racket that is strung with cord to strike a hollow rubber ball covered with felt over or around a net and into the opponent's court.\n",
      "the object of the game is to maneuver the ball in such a way that the opponent is not able to play a valid return.\n"
     ]
    }
   ],
   "source": [
    "article_sentence = nltk.sent_tokenize(article_text)\n",
    "\n",
    "for sentence in article_sentence[:3]:\n",
    "    print(sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Divide to words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tennis', 'is', 'a']\n"
     ]
    }
   ],
   "source": [
    "article_words = nltk.word_tokenize(article_text)\n",
    "print(article_words[:3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Lemmatization\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "wnlemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def perform_lemmatization(tokens):\n",
    "    return [wnlemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "\n",
    "punctuation_removal = {\n",
    "    ord(punctuation): None\n",
    "    for punctuation in string.punctuation\n",
    "}\n",
    "\n",
    "\n",
    "def get_processed_text(document: str):\n",
    "    return perform_lemmatization(nltk.word_tokenize(document.lower().translate(punctuation_removal)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{33: None, 34: None, 35: None, 36: None, 37: None, 38: None, 39: None, 40: None, 41: None, 42: None, 43: None, 44: None, 45: None, 46: None, 47: None, 58: None, 59: None, 60: None, 61: None, 62: None, 63: None, 64: None, 91: None, 92: None, 93: None, 94: None, 95: None, 96: None, 123: None, 124: None, 125: None, 126: None}\n"
     ]
    }
   ],
   "source": [
    "print(punctuation_removal)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Responding to Greetings\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "greeting_inputs =(\n",
    "    'hey',\n",
    "    'good morning',\n",
    "    'good evening',\n",
    "    'morning',\n",
    "    'evening',\n",
    "    'hi',\n",
    "    'whatsup',\n",
    ")\n",
    "\n",
    "greeting_responses = [\n",
    "    'hey',\n",
    "    'hey hows you?',\n",
    "    '*nods*',\n",
    "    'hello, how you doing',\n",
    "    'hello, how you doing',\n",
    "    'hello',\n",
    "    'you are welcome',\n",
    "]\n",
    "\n",
    "def generate_greeting_response(greeting: str) -> str:\n",
    "    for token in greeting.split():\n",
    "        if token.lower() in greeting_inputs:\n",
    "            return random.choice(greeting_responses)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Responding to User Queries\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def generate_response(user_input: str) -> str:\n",
    "    sentences = [*article_sentence, user_input]\n",
    "\n",
    "    word_vectorizer = TfidfVectorizer(\n",
    "        tokenizer=get_processed_text,\n",
    "        stop_words='english'\n",
    "    )\n",
    "\n",
    "    all_word_vectors = word_vectorizer.fit_transform(sentences)\n",
    "    similarly_vector_values = cosine_similarity(all_word_vectors[-1], all_word_vectors)\n",
    "    similar_sentence_number = similarly_vector_values.argsort()[0][-2]\n",
    "\n",
    "    matched_vector = similarly_vector_values.flatten()\n",
    "    matched_vector.sort()\n",
    "    vector_matched = matched_vector[-2]\n",
    "\n",
    "    if vector_matched == 0:\n",
    "        return \"I'm sorry, I could not understand you.\"\n",
    "    else:\n",
    "        return article_sentence[similar_sentence_number]\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashir\\anaconda3\\envs\\AIBot\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"I'm sorry, I could not understand you.\""
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response('how')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ashir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "aibot",
   "language": "python",
   "display_name": "AIBot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}